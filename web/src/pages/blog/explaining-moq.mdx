# Explaining MoQ
wtf is a moq

## Layers
We start our journey by slicing the onion.
The most critical, and least documented, thing to understand is the layering.

Without further introduction, from the bottom to top:
- QUIC: The network layer
- WebTransport: Browser compatibility 
- MoqTransfork: Media-like pub/sub.
- MoqKarp: The media layer.
- UrApp: Your application.

This guide will also help explain the IETF drafts.
I forked them to simplify the implementation, but the high level concepts are still very similar.
Just gotta rename a few things:

- Transfork -> Transport
- Karp -> Warp
- Frame -> Object*

Yes I only changed a few letters; your mental model will thank me.

## QUIC
QUIC is that new protocol that powers HTTP/3.
It's intended to be an generic improvement over TCP

If you've used TCP before (you have, don't lie) then you'll know that it's fully reliable and ordered.
It's a FIFO over the internet.

But the internet is full of rough seas.
Step outside and you'll encounter a swirling typhoon in the form of a ðŸŒ€ loading ðŸŒ€ animation.
Sometimes we don't want to wait for everything; sometimes we want to skip.

There have been many approaches fix head-of-line in HTTP:

- With HTTP/1, browsers would utilize multiple TCP connections.
However, each connection involves a relatively expensive TCP/TLS handshake and these connections then fight with each other for resources.

- With HTTP/2, browsers utilize a single, shared TCP connection to each host.
However, despite the illusion of independent requests, everything is interleaved and head-of-line blocking runs rampant.

- With HTTP/3, QUIC fixes it.

What?
How?
QUIC by provides a single connection with multiple streams.
These streams can be created, delivered, and closed in parallel with minimal overhead.
The important transport stuff is shared, like encryption, congestion control, and flow control.

Crucially for MoQ, QUIC streams are cooperative.
They don't block each other, so the order that streams are transmitted can be determined by the QUIC library.
We utilize this *prioritization* so the most important media (ex. new audio > old video) is transmitted first during congestion, although not necessarily received first.

Note that QUIC uses UDP under the hood.
It can't use TCP because of head-of-line blocking (duh) and it can't use IP because it hurts adoption (ex. SCTP).
This is mostly an implementation detail, but does cause some firewall and performance problems because the Internet is (currently) optimized for TCP.

Also, QUIC currently doesn't support peer-to-peer, so the cloud haters are stuck with WebRTC for the meantime.
Somebody else should layer QUIC on top of ICE; not me.

But why not build on top of UDP like scores of other, live media protocols?
It's pretty simple actually:
- QUIC is wicked smart: check out my QUIC POWERS blog post for more info.
- QUIC benefits from economies of scale.

We should tp build on the shoulders of smart individuals and smart (optimized) implementations.
It's near perfect already.

## WebTransport 
I just said QUIC was created for HTTP/3... why not use HTTP/3?
That's QUIC right?

Well you can totally use HTTP/3 to implement something like MoQ.
However, the HTTP semantics add more hoops to jump through, for example long-polling to emulate a live stream.
Somebody else should and will make "Media over HTTP/3"

I'm interested in WebTransport instead.
It's a browser API that exposes QUIC streams to a web application, similar to how WebSockets exposes TCP.
The connection-oriented nature makes it significantly easier to push media without the HTTP request/response song and dance.
There's not much else to say, it's basically the same thing as QUIC.

...except that the underlying implementation is gross.
WebTransport shares a QUIC connection with HTTP/3 and potentially other WebTransport sessions.
This *pooling* feature is responsible for a ton of headaches but it's too late to argue against it any louder.
There's also a TCP fallback in development (Http2Transport) but I would use WebSockets instead.

Just use an exising WebTransport library (like mine!) and pretend that it's identical to QUIC.

## MoqTransfork
Finally, the good stuff.

Remember the part where I said where don't want to use HTTP/3?
It was literally in the previous section.
Well, we do want some of the nice HTTP properties so it's time to make our own.

One of the biggest headaches with WebRTC is the story around fan out.
A broadcaster could establish a connection to all N viewers, but sending N copies of the data quickly becomes a problem especially on slow networks.
Outside of 1:1 or niche peer-to-peer use-cases, WebRTC broadcasters instead connect to an SFU server and use the power of data centers to fan media out to all viewers.

The problem with this approach is that it's surprisingly custom.
There's no generic way for an application to signal *how* media should be forwarded, leading to divergent and usually proprietary SFU behavior.
A (good) SFU is also very media specific, often parsing the payload on a per-codec basis to detect things like keyframes.
All of this custom behavior deincentivises reuse and shrinks the available offerings.

MoqTransfork is an ambitious attempt to learn from the succesess of HTTP and the affirmationed failures of WebRTC.
But you know, for live content and not *hyper-text*.

## MoqTransfork (explained)
MoqTransfork is a pub/sub protocol loosely modeled after video encoding.
However, it's generic and can be used to transport a wide variety of live content.

The idea is that generic relays and CDNs implement MoqTransfork but no higher.
There's enough information in the MoqTransfork headers to facilitate ideal caching and fanout out even in congested scenarios.
The closest analogy is HTTP: a HTTP reverse proxy knows how to forward/cache requests based on HTTP headers and ignores the body.

### Connection 
A client establishes a QUIC/WebTransport connection to a remote server and negotiates the MoqTransfork version.

Both the client and server can then publish or subscribe to tracks.
Each subscription is scoped to a single track.
The subscriber chooses the priority of each track, delegating to the publisher on a tie.

A subscriber MUST initiate a subscription; you cannot push arbitrary tracks.
However, the subscriber can (live) discover any tracks matching a provided prefix, making it easy to choose.


### Track
A track is a live series of independent groups.

That's confusing so I'm spoiling the karp section: imagine tracks such as "480p", "audio", or "captions".
A viewer might want "video" to be dropped in favor of "audio" during congestion so it's lower priority 
A viewer might not want the "caption" track so it doesn't subscribe to it.

Note that groups are *independent*.
This means you could receive the latest group before receiving all of the prior group.
In fact the subscriber can hint this behavior to the publisher: give me older, newer, or any groups first.
More on that in the Karp section.

### Group
A group is a series of dependent frames.

This one is easy: frames within a group usually dependent on prior frames.
Another word for this is "delta encoded". There's no benefit in delivering these frames out of order because they cannot be decoded out of order.

A MoqTransfork group maps 1:1 with a QUIC stream.
This is the biggest difference between the IETF draft and my fork.
I'm not even going to attempt to explain that draft's convoluted usage of QUIC.

### Frame
Literally just bytes.
Well and an upfront size.

## Karp
Okay okay, so it's finally time for the big reveal.
Karp is a layer on top of MoqTransfork that actually does the Media in MoQ.
It's also the simplest.
Wowee.

Karp is not intended to support every feature for every application for the end of time.
It's meant to be the base functionality for generic applications (ex. OBS, VLC) to interface with generic services (ex. YouTube, Twitch).
The point of standards, and layering in general, is reusability and we *should* be selective about what makes the cut.

Karp consists of a JSON "catalog" and a custom media container.
The contents of both are actually modeled after the WebCodecs API; you have exactly the needed information to initialize the decoder and process each frame.

### Catalog
The catalog is a description of tracks within a broadcast.
A viewer uses this metadata to decide if it wants a track and how to decode it.

For example, a "480p" track:
```
```

The catalog also contains the relationship between tracks.
For example, two tracks could have the same content but different settings, like "480p" vs "1080p".
A viewer should not subscribe to both of them and instead switch between them based on the indicated max bitrate.

The catalog is very similar to a HLS/DASH playlist but it doesn't reek of Apple or XML.
But it's also missing a ton of features and corresponding bloat.
It's also similar to SDP from WebRTC but that format needs to be in hospice care already.

The catalog itself is delivered as a MoqTransfork track.
This might seem unnecessary until you realize that it now automatically supports *live updates*.
Add or remove a track from the catalog and all subscribed viewers get the update free of charge.
This is unironically a huge deal as it fixes a common frustration with the afformentioned playlist formats 

Note that a catalog is not strictly required because track names are discoverable.
But encoding information into the track name is extremely brittle and you should feel bad for doing it.
The catalog is also a good place to include initialization information like the H.264 SPS/PPS instead of inline.

### Media Container 
Originally, Warp used fMP4 segments.
This is great for a company like Twitch who already uses fMP4 (aka CMAF) for HLS/DASH delivery.

Unfortunately, this container is not designed for live streaming.
You can minimize latency by fragmenting (the f in fMP4) at each frame but this involves ~100 bytes of overhead. 
In fact, half of that consists of kind/size pairs to construct the unnecessary XML-like boxes.
 
This is nearly the size of OPUS audio packet and doubling our network usage for audio-only streams is unacceptable.

So let's build our own container!
It consists of:
- A 1-8 byte presentation timestamp
- A payload

That's it.
The rest of the information, like the track ID and if this is a keyframe, is part of the MoqTransfork layer.

Obviously we'll need more information (ex. keying information) in the future so expect to see updates on this front.


## UrApp 
You're on your own now.
Fly free little developer.

Of course there are libraries to help the transition.
Nobody is expecting you to write your own QUIC, Transfork, or Karp implementation.
But you could, and you should, if extensibility is more important for your application than compatibility.

But how do you extend MoQ?
Well of course it depends on the layer, perhaps:

- A QUIC extension.
- A WebTransport extension.
- A MoqTransfork extension.
- A MoqTransfork track.
- A MoqKarp extension.

One of those in particular should stick out: you can create arbitrary tracks.
For example, `controller` or `chat`.
You do need to figure out how to delta encode your content into Groups and Frames but that can be straightforward.

This functionality is key because your application might not have control of other layers in the stack.
If you're using a browser or QUIC LB, you can't exactly make your own QUIC extension.
If you're using a 3rd party MoQ CDN (one day...), you can't exactly change MoqTransfork.
But you should always be able to create your own tracks that live alongside your media.

## Forewarning 
MoQ is evolving rapidly.
The core concepts are stable but everything else keeps evolving, even the terminology.

Evolve with me.
Platonically of course.

@kixelated
