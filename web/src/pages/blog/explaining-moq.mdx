---
layout: "@/layouts/global.astro"
title: MoQ is an Onion
author: kixelated
description: Media over Transfork over WebTransport over QUIC over UDP over IP over Ethernet over Fiber over Light over Space over Time
cover: "/blog/forward-error-correction/mfw.jpeg"
date: 2024-11-13
---

# MoQ Layers
Today we slice the onion.
The most critical, and least documented, thing to understand is the layering behind MoQ.

Without further blabbering, from the bottom to top:
- **QUIC**: The network layer
- **Web Transport**: Browser compatibility
- **MoQ Transfork**: Media-like pub/sub.
- **MoQ Karp**: A media playlist and container.
- **Ur App**: Your application.

This layering is the most crucial concept behind MoQ, believe it or not.
We explicitly want to avoid building yet another monolithic and inflexible media protocol built directly on top of UDP.
Sorry SRT, you're not invited to this party.

This guide will also help explain the IETF drafts.
Of course I decided to [fork them](./transfork), but the high level concepts are still very similar.
Just gotta rename a few things:

- [Transfork](https://datatracker.ietf.org/doc/draft-lcurley-moq-transfork/) -> [Transport](https://datatracker.ietf.org/doc/draft-ietf-moq-transport/)
- Karp -> [Warp](https://datatracker.ietf.org/doc/draft-law-moq-warpstreamingformat/)

Yes, I used unoriginal names on purpose.
Your mental model should thank me.

## QUIC
QUIC is that new protocol that powers HTTP/3.
Itâ€™s designed as a general improvement over TCP.

If you've used TCP before (you have, don't lie) then you'll know that it's fully reliable and ordered.
It's a FIFO over the internet.
But the internet is full of rough seas.
Step outside and you'll encounter a swirling typhoon in the form of a ðŸŒ€ loading ðŸŒ€ animation.
Sometimes we don't want to wait for everything; sometimes we want to skip.

There have been many attempts to fix head-of-line in HTTP:

- With HTTP/1, browsers would utilize multiple TCP connections.
However, each connection involves a relatively expensive TCP/TLS handshake.
These connections will then fight with each other for resources, creating a delicate balancing act if you want to prioritize requests.

- With HTTP/2, browsers utilize a single, shared TCP connection to each host.
However, despite the illusion of independent requests and a complex prioritization scheme, it all gets interleaved into a single pipe.
The result is rampant head-of-line blocking.

- With HTTP/3, QUIC fixes it.
What?
How?

QUIC combines the two approaches by sharing some state (like HTTP/2) while providing independent streams (like HTTP/1).
Each HTTP request is a QUIC stream, which can be created, delivered, and closed in parallel with minimal overhead.
All of the encryption, congestion control, and flow control is shared at the connection level.

But we're not using QUIC for HTTP.
We're using QUIC because crucially, QUIC streams are cooperative.
The order that streams are transmitted can be determined by the QUIC library since they don't block each other.
We utilize this *prioritization* such the most important media (ex. new audio > old video) is transmitted first during congestion, although not necessarily received first.

But why not build on top of UDP like scores of other, live media protocols?
It's pretty simple actually:
- QUIC is wicked smart: check out my QUIC POWERS blog post for more info.
- QUIC benefits from economies of scale.

We should tp build on the shoulders of smart individuals and smart (optimized) implementations.
[It's near perfect already](./quic-powers).

## Web Transport
I just said QUIC was created for HTTP/3... why not use HTTP/3?
That's QUIC right?

Well you can totally use HTTP/3 to implement something like MoQ.
However, the HTTP semantics add more hoops to jump through, for example long-polling to emulate a live stream.
Somebody else should and will make "Media over HTTP/3"

I'm interested in [WebTransport](https://developer.mozilla.org/en-US/docs/Web/API/WebTransport_API) instead.
It's a browser API that exposes QUIC streams to a web application, similar to how WebSockets exposes TCP.
The connection-oriented nature makes it significantly easier to push media without the HTTP request/response song and dance.
There's not much else to say, it's basically the same thing as QUIC.

...except that the underlying implementation is gross.
A WebTransport session shares a QUIC connection with HTTP/3 requests and potentially other WebTransport sessions ([spec](https://datatracker.ietf.org/doc/draft-ietf-webtrans-http3/spec)).
This *pooling* feature is responsible for a ton of headaches but the IETF went ahead and standardized it anyway (despite my best efforts).

So we use WebTransport for WebSupport.
Choose a WebTransport library ([like mine!](https://docs.rs/web-transport-quinn/latest/web_transport_quinn/)) and pretend it's just QUIC.

Note: There is a [HTTP/2 variant of WebTransport](https://datatracker.ietf.org/doc/draft-ietf-webtrans-http2/) intended as a fallback, but it's unsupported as of late and is going to suffer from head-of-line blocking.
IMO just use WebSockets instead.

## Learnings
Instead of explaining my favorite protocol, sometimes it's easier to learn from other protocols.

Specifically, one of the biggest headaches with WebRTC is how to handle 1:N fanout.
It's something you take for granted with HTTP: just throw a CDN in front of it.

The naive WebRTC approach is to have a broadcaster establish a connection to all N viewers, sending N copies of the media.
This can work for low-bitrate applications (ie. audio) but obviously doesn't scale and is not used outside of 1:1 or peer-to-peer scenarios.

Instead, the participants connect to an server (called an "SFU") and use the power of data centers to distribute the media.
Sure our cloud bill is higher but now we can have more viewers than fingers.
We also benefit from peering agreements and other CDN magic, because it turns out that ISPs really don't like globally dispersed P2P traffic (aka bittorrent).

The problem with this approach is that it's unfortunately custom.
There's no generic way for an application to signal *how* media should be forwarded, leading to divergent and proprietary behavior.
Every service has it's own priopritary SFU and the line between application and transport is extremely blurry.
For example, a good SFU will parse each packet on a per-codec basis to detect things like keyframes, but most won't.

## Transfork
So let's fix it.
MoqTransfork is an ambitious attempt to learn from the succesess of HTTP and the aforementioned failures of WebRTC.
But you know, for live content and not *hyper-text*.

Specifically, we want a distinct transport layer that can be reused across applications.
This layer should have the appropriate semantics and metadata to facilitate caching and fanout.
And it should also be generic enough to support a wide variety of live content.

The idea is that generic relays and CDNs implement [MoQ Transfork](https://datatracker.ietf.org/doc/draft-lcurley-moq-transfork/) but no higher.
There's enough information in the MoqTransfork headers to facilitate ideal caching and fanout out even in congested scenarios.
The closest analogy is HTTP: a HTTP reverse proxy knows how to forward/cache requests based on HTTP headers and ignores the body.

But how does it work?
MoqTransfork allows a client to publish or subscribe to live "tracks" by name.
A track is a series of independent groups, which are a series of dependent frames.
That probably went over your head so let me explain.

Video encoding is a form of delta encoding.
The simplest encoding involves a periodic I-frame (basically a static image) followed by a series of P-frames, each encoded as differences from the prior frame(s).
There are more complicated encoding schemes but let's stick with this for now.

To avoid artifacts, we explicitly don't want to decode a frame without all of its dependencies.
And yet we also want the ability to skip ahead to the newest frames during congestion (avoiding the typhoon).
So we have but one option, to split the live stream at I-frame boundaries and deliver P-frames in order.

This is known as a "Group of Pictures" aka a "GOP" aka a "HLS/DASH segment".
It's called a "Group" in Transfork just to throw one more word into the salad.
Each Group is delivered over a dedicated QUIC stream so each Frame is delivered reliably an in order, unless the QUIC stream is cancelled.
A subscriber will SUBSCRIBE to a track and ask the publisher to prioritize newer groups/streams, dropping older ones in the process.

And just like that we have a generic, live media transport layer that can match WebRTC in terms of latency.
Obviously there's more to it than that (see other blog posts) but the high level concept is surprisingly simple.

## Karp
Okay okay, so it's finally time for the big reveal: the M in MoQ.
Karp is a layer on top of MoqTransfork that actually does the Media stuff.
It's also the simplest.
Wowee.

Karp is not intended to support every feature for every application for the end of time.
It's meant to be the base functionality for generic applications (ex. OBS, VLC) to interface with generic services (ex. YouTube, Twitch).
The point of standards, and layering in general, is to promote reusability.
We *should* be selective about what makes the cut lest the standard becomes a burden.

Karp is modeled after the [WebCodecs API](https://developer.mozilla.org/en-US/docs/Web/API/WebCodecs_API).
The application has exactly the needed information to initialize the decoder and process each frame.

Karp consists of:
- A **catalog**: A JSON blob describing the tracks.
- A **container**: A simple header in front of each frame (codec bitstream).

The catalog is delivered over a Transfork track, commonly named `catalog.json`.
It contains metadata a viewer will use to select desired tracks and initialize the decoder.
Let's just dive into an example, here's a fragment that describes a video track:

```JSON
{
	"track": {
		"name": "480p",
		"priority": 2
	},
	"codec": "avc1.64001f",
	"description": "<SPS/PPS>",
	"resolution": {
		"width": 1280,
		"height": 720
	},
	"bitrate": 3000000
}
```

If you've had to parse a SDP, DASH, or HLS manifest before, you'll know that this is a breath of fresh air.
And what's even cooler, this catalog is *live* and can be updated it on the fly unlike the crummy, afformentioned formats.

The catalog can contain multiple tracks, audio and video of course but also alternative renditions.
For example, two tracks could have the same content but different settings, like "480p" vs "1080p".
A viewer should not subscribe to both of them and instead switch between them based on the indicated max bitrate.

The media container is even less interesting.
Originally, Warp used fMP4 segments.
This is great for a company like Twitch who already uses fMP4 (aka CMAF) for HLS/DASH delivery.

Unfortunately, this container is not designed for live streaming.
You can minimize latency by fragmenting (the f in fMP4) at each frame but this involves ~100 bytes of overhead.
In fact, half of that consists of kind/size pairs to construct the unnecessary XML-like boxes.
This is nearly the size of OPUS audio packet and doubling our network usage for audio-only streams is unacceptable.

So we're not using fMP4, we're using our own container.
It consists of:
- A 1-8 byte presentation timestamp
- A payload

That's it.
That's it!

Obviously we'll need more functionality in the future so expect to see updates on Karp.
For example, keying information used for encryption or DRM (not the same thing lul).
But our goal is to keep it simple.

## UrApp
MoQ can provide video and audio delivery.
You're on your own for the rest.
Fly free little developer.

Of course there are libraries to help.
Nobody is expecting you to write your own QUIC, Transfork, or Karp implementation.
Likewise I expect there will be other layers on top of MoqTransfork, for example a chat protocol.

But ultimately, you're responsible for your application.
It shouldn't be generic and neither should MoQ be one-size-fits-all.
You will inevitably need to extend MoQ to fit your needs and I don't want that to involve clicking the "Feature Request" button.

But how do you extend MoQ?
Well of course it depends on the layer, perhaps:

- A QUIC extension?
- A WebTransport extension?
- A MoqTransfork extension?
- A MoqTransfork track.
- A MoqKarp extension?

One of those in particular should stick out: you can create arbitrary tracks.
For example, could create a custom `controller` or `chat` track alongside the Karp tracks.
You get all of the benefits of MoqTransfork, like prioritization and caching, without having to reinvent the wheel.
But you do need to figure out how to delta encode your content into Groups and Frames.

For example, the MoQ relays tracks to gossip available broadcasts and routes.
Each node creates a `cluster.<node>` track and subscribes to all other `cluster.*` tracks.
The track is just a series of +1 or -1 deltas, indicating when a broadcast was started or stopped.
Dog fooding to the max.

You too can abuse MoQ to deliver non-media content.
Perhaps one day this blog post will be delivered over MoQ too...

## Out of Date
MoQ is evolving rapidly.
The core concepts are stable but everything else keeps evolving, even the terminology.
I'm sure this blog post is already out of date.

[Join the conversation](https://discord.gg/FCYF3p99mr) and evolve with me, platonically of course.

Written by [@kixelated](https://github.com/kixelated).

<img src="/blog/kixelCat.png" class="inline w-16" />
